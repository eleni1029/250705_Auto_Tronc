import os
import glob
import pandas as pd
from datetime import datetime
import re

from sub_todolist_result import process_sheet_data
from sub_todolist_resource import extract_resources_from_result, get_resource_statistics
import json

def get_analyzed_files():
    """ç²å– to_be_executed ç›®éŒ„ä¸­æ‰€æœ‰ analyzed æª”æ¡ˆï¼ŒæŒ‰æ™‚é–“æ’åº"""
    pattern = os.path.join('to_be_executed', 'course_structures_analyzed_*.xlsx')
    files = glob.glob(pattern)
    
    if not files:
        print("âŒ åœ¨ to_be_executed ç›®éŒ„ä¸­æ‰¾ä¸åˆ° analyzed æª”æ¡ˆ")
        return []
    
    # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    files.sort(key=os.path.getmtime, reverse=True)
    return files

def extract_timestamp(filename):
    """å¾æª”æ¡ˆåä¸­æå–æ™‚é–“æˆ³"""
    match = re.search(r'analyzed_(\d{8}_\d{6})', filename)
    if match:
        return match.group(1)
    return "unknown"

def select_file(files):
    """è®“ç”¨æˆ¶é¸æ“‡æª”æ¡ˆ - GUIçµ‚ç«¯å‹å–„ç‰ˆæœ¬"""
    print("\nğŸ“ æ‰¾åˆ°ä»¥ä¸‹ analyzed æª”æ¡ˆï¼ˆæŒ‰æ™‚é–“æ’åºï¼Œæœ€æ–°çš„åœ¨å‰ï¼‰ï¼š")
    for i, file in enumerate(files, 1):
        timestamp = extract_timestamp(file)
        print(f"{i}. {os.path.basename(file)} (æ™‚é–“æˆ³: {timestamp})")
    
    print("\nğŸ“ é¸æ“‡èªªæ˜ï¼š")
    print(f"- è¼¸å…¥ 1-{len(files)} ä¹‹é–“çš„æ•¸å­—é¸æ“‡ç‰¹å®šæª”æ¡ˆ")
    print("- è¼¸å…¥ '0' ä½¿ç”¨æœ€æ–°çš„æª”æ¡ˆ (é è¨­)")
    
    while True:
        try:
            print(f"\nè«‹é¸æ“‡æª”æ¡ˆ (1-{len(files)}) æˆ–è¼¸å…¥'0'ä½¿ç”¨é è¨­:")
            print(">>> ", end="", flush=True)
            choice = input().strip()
            if not choice:
                print("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆå€¼ï¼Œæˆ–è¼¸å…¥ '0' ä½¿ç”¨é è¨­å€¼")
                continue
            if choice == '0':
                print(f"\nâœ… å·²é¸æ“‡é è¨­æª”æ¡ˆ: {os.path.basename(files[0])}")
                return files[0]  # é è¨­é¸æ“‡æœ€æ–°çš„
            
            choice_num = int(choice)
            if 1 <= choice_num <= len(files):
                print(f"\nâœ… å·²é¸æ“‡æª”æ¡ˆ: {os.path.basename(files[choice_num - 1])}")
                return files[choice_num - 1]
            else:
                print(f"âŒ è«‹è¼¸å…¥ 1-{len(files)} ä¹‹é–“çš„æ•¸å­—")
        except ValueError:
            print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—")

def get_sheet_names(file_path):
    """ç²å– Excel æª”æ¡ˆä¸­çš„æ‰€æœ‰ sheet åç¨±"""
    try:
        excel_file = pd.ExcelFile(file_path)
        return excel_file.sheet_names
    except Exception as e:
        print(f"âŒ è®€å– Excel æª”æ¡ˆæ™‚å‡ºéŒ¯: {e}")
        return []

def analyze_duplicate_names(all_result_data):
    """åˆ†æé‡è¤‡åç¨±çµ±è¨ˆ"""
    duplicate_stats = {
        'courses': {'total': 0, 'duplicates': [], 'unique_originals': 0},
        'chapters': {'total': 0, 'duplicates': [], 'unique_originals': 0},
        'units': {'total': 0, 'duplicates': [], 'unique_originals': 0}
    }
    
    # åˆ†æèª²ç¨‹é‡è¤‡
    course_originals = {}
    for item in all_result_data:
        if item['é¡å‹'] == 'èª²ç¨‹':
            course_name = item['åç¨±']
            # æª¢æŸ¥æ˜¯å¦ç‚ºç·¨è™Ÿç‰ˆæœ¬
            if '_' in course_name and course_name.split('_')[-1].isdigit():
                original_name = '_'.join(course_name.split('_')[:-1])
            else:
                original_name = course_name
            
            if original_name not in course_originals:
                course_originals[original_name] = []
            course_originals[original_name].append(course_name)
    
    duplicate_stats['courses']['unique_originals'] = len(course_originals)
    for original, versions in course_originals.items():
        if len(versions) > 1:
            duplicate_stats['courses']['duplicates'].append({
                'original': original,
                'versions': versions,
                'count': len(versions)
            })
    duplicate_stats['courses']['total'] = sum(len(v) for v in course_originals.values())
    
    # åˆ†æç« ç¯€é‡è¤‡ï¼ˆæŒ‰èª²ç¨‹åˆ†çµ„ï¼‰
    chapter_by_course = {}
    for item in all_result_data:
        if item['é¡å‹'] == 'ç« ç¯€':
            course_name = item['æ‰€å±¬èª²ç¨‹']
            chapter_name = item['åç¨±']
            
            if course_name not in chapter_by_course:
                chapter_by_course[course_name] = {}
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºç·¨è™Ÿç‰ˆæœ¬
            if '_' in chapter_name and chapter_name.split('_')[-1].isdigit():
                original_name = '_'.join(chapter_name.split('_')[:-1])
            else:
                original_name = chapter_name
            
            if original_name not in chapter_by_course[course_name]:
                chapter_by_course[course_name][original_name] = []
            chapter_by_course[course_name][original_name].append(chapter_name)
    
    total_chapters = 0
    for course, chapters in chapter_by_course.items():
        for original, versions in chapters.items():
            total_chapters += len(versions)
            if len(versions) > 1:
                duplicate_stats['chapters']['duplicates'].append({
                    'course': course,
                    'original': original,
                    'versions': versions,
                    'count': len(versions)
                })
    duplicate_stats['chapters']['total'] = total_chapters
    duplicate_stats['chapters']['unique_originals'] = sum(len(chapters) for chapters in chapter_by_course.values())
    
    # åˆ†æå–®å…ƒé‡è¤‡ï¼ˆæŒ‰èª²ç¨‹å’Œç« ç¯€åˆ†çµ„ï¼‰
    unit_by_course_chapter = {}
    for item in all_result_data:
        if item['é¡å‹'] == 'å–®å…ƒ':
            course_name = item['æ‰€å±¬èª²ç¨‹']
            chapter_name = item['æ‰€å±¬ç« ç¯€']
            unit_name = item['åç¨±']
            
            key = (course_name, chapter_name)
            if key not in unit_by_course_chapter:
                unit_by_course_chapter[key] = {}
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºç·¨è™Ÿç‰ˆæœ¬
            if '_' in unit_name and unit_name.split('_')[-1].isdigit():
                original_name = '_'.join(unit_name.split('_')[:-1])
            else:
                original_name = unit_name
            
            if original_name not in unit_by_course_chapter[key]:
                unit_by_course_chapter[key][original_name] = []
            unit_by_course_chapter[key][original_name].append(unit_name)
    
    total_units = 0
    for (course, chapter), units in unit_by_course_chapter.items():
        for original, versions in units.items():
            total_units += len(versions)
            if len(versions) > 1:
                duplicate_stats['units']['duplicates'].append({
                    'course': course,
                    'chapter': chapter,
                    'original': original,
                    'versions': versions,
                    'count': len(versions)
                })
    duplicate_stats['units']['total'] = total_units
    duplicate_stats['units']['unique_originals'] = sum(len(units) for units in unit_by_course_chapter.values())
    
    return duplicate_stats

def select_sheet(sheet_names):
    """è®“ç”¨æˆ¶é¸æ“‡ sheet - GUIçµ‚ç«¯å‹å–„ç‰ˆæœ¬"""
    print(f"\nğŸ“‹ æª”æ¡ˆåŒ…å«ä»¥ä¸‹ {len(sheet_names)} å€‹ sheet:")
    for i, sheet in enumerate(sheet_names, 1):
        print(f"{i}. {sheet}")
    
    print("\nğŸ“ é¸æ“‡èªªæ˜ï¼š")
    print(f"- è¼¸å…¥ 1-{len(sheet_names)} ä¹‹é–“çš„æ•¸å­—é¸æ“‡å–®å€‹ sheet")
    print("- è¼¸å…¥å¤šå€‹æ•¸å­—ä¸¦ç”¨é€—è™Ÿåˆ†éš”é¸æ“‡å¤šå€‹ sheet")
    print("  ä¾‹å¦‚: 1,3,5 (é¸æ“‡ç¬¬1ã€3ã€5å€‹sheet)")
    print("  ä¾‹å¦‚: 2,4,6,8 (é¸æ“‡ç¬¬2ã€4ã€6ã€8å€‹sheet)")
    print("- è¼¸å…¥ 'all' é¸æ“‡æ‰€æœ‰ sheet")
    
    while True:
        try:
            # åˆ†é›¢æç¤ºè¨Šæ¯å’Œè¼¸å…¥ï¼Œç¢ºä¿æç¤ºè¨Šæ¯èƒ½é¡¯ç¤º
            print(f"\nè«‹é¸æ“‡ sheet (1-{len(sheet_names)})ã€å¤šå€‹æ•¸å­—ç”¨é€—è™Ÿåˆ†éš”ã€æˆ–è¼¸å…¥'all':")
            print(">>> ", end="", flush=True)
            choice = input().strip().lower()
            
            if choice == 'all':
                print(f"\nâœ… å·²é¸æ“‡æ‰€æœ‰ sheet: {len(sheet_names)} å€‹")
                return sheet_names
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«é€—è™Ÿï¼ˆå¤šé¸æ¨¡å¼ï¼‰
            if ',' in choice:
                # è§£æå¤šå€‹æ•¸å­—
                numbers = []
                selected_sheets = []
                error_occurred = False
                
                for num_str in choice.split(','):
                    num_str = num_str.strip()
                    if not num_str:
                        continue
                    
                    try:
                        num = int(num_str)
                        if 1 <= num <= len(sheet_names):
                            if num not in numbers:  # é¿å…é‡è¤‡é¸æ“‡
                                numbers.append(num)
                                selected_sheets.append(sheet_names[num - 1])
                        else:
                            print(f"âŒ æ•¸å­— {num} è¶…å‡ºç¯„åœ (1-{len(sheet_names)})")
                            error_occurred = True
                            break
                    except ValueError:
                        print(f"âŒ '{num_str}' ä¸æ˜¯æœ‰æ•ˆçš„æ•¸å­—")
                        error_occurred = True
                        break
                
                if error_occurred:
                    continue
                
                if selected_sheets:
                    print(f"\nâœ… å·²é¸æ“‡ {len(selected_sheets)} å€‹ sheet:")
                    for i, sheet in enumerate(selected_sheets, 1):
                        print(f"  {i}. {sheet}")
                    return selected_sheets
                else:
                    print("âŒ æ²’æœ‰é¸æ“‡ä»»ä½•æœ‰æ•ˆçš„ sheet")
                    continue
            
            else:
                # å–®é¸æ¨¡å¼
                choice_num = int(choice)
                if 1 <= choice_num <= len(sheet_names):
                    selected_sheet = sheet_names[choice_num - 1]
                    print(f"\nâœ… å·²é¸æ“‡ sheet: {selected_sheet}")
                    return [selected_sheet]
                else:
                    print(f"âŒ è«‹è¼¸å…¥ 1-{len(sheet_names)} ä¹‹é–“çš„æ•¸å­—ï¼Œæˆ–è¼¸å…¥ 'all'")
        except ValueError:
            print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—ã€ç”¨é€—è™Ÿåˆ†éš”çš„å¤šå€‹æ•¸å­—ã€æˆ– 'all'")

def analyze_course_statistics(all_result_data, all_resource_data, timestamp):
    """
    åˆ†æå„èª²ç¨‹çµ±è¨ˆæ•¸æ“šä¸¦ç”Ÿæˆè©³ç´°å ±å‘Š
    
    Args:
        all_result_data: æ‰€æœ‰çµæœæ•¸æ“š
        all_resource_data: æ‰€æœ‰è³‡æºæ•¸æ“š
        timestamp: æ™‚é–“æˆ³
    """
    # HTML æ“´å±•åé…ç½®ï¼ˆèˆ‡ sub_mp4filepath_identifier.py ä¿æŒä¸€è‡´ï¼‰
    HTML_EXTENSIONS = ['.html', '.htm']
    
    # åˆ†æé‡è¤‡åç¨±çµ±è¨ˆ
    duplicate_stats = analyze_duplicate_names(all_result_data)
    
    # æŒ‰èª²ç¨‹åˆ†çµ„çµ±è¨ˆ
    course_stats = {}
    course_order = []
    
    # å»ºç«‹èª²ç¨‹èˆ‡ä¾†æºSheetçš„æ˜ å°„
    course_to_sheet = {}
    
    # çµ±è¨ˆå„èª²ç¨‹çš„åŸºæœ¬æ•¸æ“š
    for item in all_result_data:
        course_name = item['æ‰€å±¬èª²ç¨‹']
        if not course_name or course_name == 'nan':
            continue
            
        # è¨˜éŒ„èª²ç¨‹èˆ‡ä¾†æºSheetçš„å°æ‡‰é—œä¿‚
        if course_name not in course_to_sheet and item.get('ä¾†æºSheet'):
            course_to_sheet[course_name] = item['ä¾†æºSheet']
            
        if course_name not in course_stats:
            course_stats[course_name] = {
                'chapters': set(),
                'units': set(), 
                'activities': [],
                'resources': set(),
                'html_files': 0,
                'video_activities': 0  # ä¿®æ­£ï¼šç›´æ¥çµ±è¨ˆå½±éŸ³æ•™ææ´»å‹•æ•¸é‡
            }
            course_order.append(course_name)
        
        stats = course_stats[course_name]
        
        if item['é¡å‹'] == 'ç« ç¯€':
            stats['chapters'].add(item['åç¨±'])
        elif item['é¡å‹'] == 'å–®å…ƒ':
            stats['units'].add(item['åç¨±'])
        elif item['é¡å‹'] == 'å­¸ç¿’æ´»å‹•':
            stats['activities'].append(item)
            
            # çµ±è¨ˆè³‡æº
            if item['æª”æ¡ˆè·¯å¾‘']:
                stats['resources'].add(item['æª”æ¡ˆè·¯å¾‘'])
            
            # ä¿®æ­£1ï¼šçµ±è¨ˆHTMLç·šä¸Šé€£çµæ•¸é‡ï¼ˆæª¢æŸ¥ç¶²å€è·¯å¾‘ï¼‰
            if item['ç¶²å€è·¯å¾‘']:
                # æª¢æŸ¥ç¶²å€è·¯å¾‘ä¸­æ˜¯å¦åŒ…å«HTMLæª”æ¡ˆ
                web_path = str(item['ç¶²å€è·¯å¾‘']).lower()
                if any(ext.lower() in web_path for ext in HTML_EXTENSIONS):
                    stats['html_files'] += 1
            
            # ä¿®æ­£2ï¼šç›´æ¥çµ±è¨ˆå½±éŸ³æ•™ææ´»å‹•æ•¸é‡ï¼Œä¸ä¾è³´æª”æ¡ˆå‰¯æª”å
            if item['å­¸ç¿’æ´»å‹•é¡å‹'] in ['å½±éŸ³æ•™æ_å½±ç‰‡', 'å½±éŸ³æ•™æ_éŸ³è¨Š']:
                stats['video_activities'] += 1
    
    # ç”Ÿæˆæ—¥èªŒæª”æ¡ˆ
    log_filename = f"log/todolist_course_analysis_{timestamp}.log"
    os.makedirs("log", exist_ok=True)
    
    # å¾ all_resource_data ä¸­æå–é‡è¤‡å¼•ç”¨çš„è³‡æºçµ±è¨ˆ
    # å»ºç«‹æª”æ¡ˆè·¯å¾‘åˆ°å¼•ç”¨æ¬¡æ•¸çš„æ˜ å°„
    file_path_references = {}
    for resource in all_resource_data:
        file_path = resource['æª”æ¡ˆè·¯å¾‘']
        # å¾ all_result_data ä¸­è¨ˆç®—å¯¦éš›å¼•ç”¨æ¬¡æ•¸
        reference_count = sum(1 for item in all_result_data 
                            if item['é¡å‹'] == 'å­¸ç¿’æ´»å‹•' and item['æª”æ¡ˆè·¯å¾‘'] == file_path)
        file_path_references[file_path] = reference_count
    
    # çµ±è¨ˆå…¨å±€é‡è¤‡å¼•ç”¨è³‡æº
    multiple_reference_resources = [
        {'æª”æ¡ˆè·¯å¾‘': path, 'å¼•ç”¨æ¬¡æ•¸': count} 
        for path, count in file_path_references.items() if count > 1
    ]
    total_saved_uploads = sum(r['å¼•ç”¨æ¬¡æ•¸'] - 1 for r in multiple_reference_resources)
    
    # ç”Ÿæˆçµ±è¨ˆå ±å‘Š
    report_data = {
        'generation_time': datetime.now().isoformat(),
        'timestamp': timestamp,
        'total_courses': len(course_stats),
        'overall_summary': {
            'total_chapters': sum(len(stats['chapters']) for stats in course_stats.values()),
            'total_units': sum(len(stats['units']) for stats in course_stats.values()),
            'total_activities': sum(len(stats['activities']) for stats in course_stats.values()),
            'total_unique_resources': len(set().union(*[stats['resources'] for stats in course_stats.values()])),
            'total_html_files': sum(stats['html_files'] for stats in course_stats.values()),
            'total_video_activities': sum(stats['video_activities'] for stats in course_stats.values()),  # ä¿®æ­£
            'multiple_reference_resources': len(multiple_reference_resources),
            'saved_uploads': total_saved_uploads
        },
        'course_details': []
    }
    
    # é€èª²ç¨‹çµ±è¨ˆ
    with open(log_filename, 'w', encoding='utf-8') as log_file:
        log_file.write("="*80 + "\n")
        log_file.write("TronClass èª²ç¨‹çµæ§‹æå–çµ±è¨ˆå ±å‘Š\n")
        log_file.write("="*80 + "\n")
        log_file.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        log_file.write(f"æ™‚é–“æˆ³: {timestamp}\n")
        log_file.write(f"ç¸½èª²ç¨‹æ•¸: {len(course_stats)}\n\n")
        
        # ç¸½é«”åŒ¯ç¸½
        log_file.write("ç¸½é«”åŒ¯ç¸½:\n")
        log_file.write("-"*40 + "\n")
        log_file.write(f"ç¸½ç« ç¯€æ•¸: {report_data['overall_summary']['total_chapters']}\n")
        log_file.write(f"ç¸½å–®å…ƒæ•¸: {report_data['overall_summary']['total_units']}\n") 
        log_file.write(f"ç¸½å­¸ç¿’æ´»å‹•æ•¸: {report_data['overall_summary']['total_activities']}\n")
        log_file.write(f"ç¸½å”¯ä¸€è³‡æºæ•¸: {report_data['overall_summary']['total_unique_resources']}\n")
        log_file.write(f"ç¸½HTMLç·šä¸Šé€£çµæ•¸: {report_data['overall_summary']['total_html_files']}\n")
        log_file.write(f"ç¸½å½±éŸ³æ•™ææ´»å‹•æ•¸: {report_data['overall_summary']['total_video_activities']}\n")  # ä¿®æ­£
        log_file.write(f"è¢«å¤šå€‹å­¸ç¿’æ´»å‹•å¼•ç”¨çš„è³‡æºæ•¸: {report_data['overall_summary']['multiple_reference_resources']}\n")
        log_file.write(f"ç¯€çœé‡è¤‡ä¸Šå‚³çš„è³‡æºæ•¸: {report_data['overall_summary']['saved_uploads']}\n\n")
        
        # é‡è¤‡åç¨±çµ±è¨ˆ
        log_file.write("é‡è¤‡åç¨±è™•ç†çµ±è¨ˆ:\n")
        log_file.write("-"*40 + "\n")
        log_file.write(f"èª²ç¨‹é‡è¤‡: {len(duplicate_stats['courses']['duplicates'])} çµ„é‡è¤‡ï¼Œå…± {duplicate_stats['courses']['total']} å€‹èª²ç¨‹\n")
        log_file.write(f"ç« ç¯€é‡è¤‡: {len(duplicate_stats['chapters']['duplicates'])} çµ„é‡è¤‡ï¼Œå…± {duplicate_stats['chapters']['total']} å€‹ç« ç¯€\n")
        log_file.write(f"å–®å…ƒé‡è¤‡: {len(duplicate_stats['units']['duplicates'])} çµ„é‡è¤‡ï¼Œå…± {duplicate_stats['units']['total']} å€‹å–®å…ƒ\n")
        
        if duplicate_stats['courses']['duplicates']:
            log_file.write("\né‡è¤‡èª²ç¨‹è©³æƒ…:\n")
            for dup in duplicate_stats['courses']['duplicates']:
                log_file.write(f"  - {dup['original']}: {', '.join(dup['versions'])}\n")
        
        if duplicate_stats['chapters']['duplicates']:
            log_file.write("\né‡è¤‡ç« ç¯€è©³æƒ…:\n")
            for dup in duplicate_stats['chapters']['duplicates']:
                log_file.write(f"  - {dup['course']} > {dup['original']}: {', '.join(dup['versions'])}\n")
        
        if duplicate_stats['units']['duplicates']:
            log_file.write("\né‡è¤‡å–®å…ƒè©³æƒ…:\n")
            for dup in duplicate_stats['units']['duplicates']:
                log_file.write(f"  - {dup['course']} > {dup['chapter']} > {dup['original']}: {', '.join(dup['versions'])}\n")
        
        log_file.write("\n")
        
        # å„èª²ç¨‹è©³ç´°çµ±è¨ˆ
        log_file.write("å„èª²ç¨‹è©³ç´°çµ±è¨ˆ:\n")
        log_file.write("="*80 + "\n")
        
        # é¡¯ç¤ºé‡è¤‡åç¨±çµ±è¨ˆ
        if (duplicate_stats['courses']['duplicates'] or 
            duplicate_stats['chapters']['duplicates'] or 
            duplicate_stats['units']['duplicates']):
            print(f"\nğŸ”„ é‡è¤‡åç¨±è™•ç†çµ±è¨ˆ:")
            print(f"  èª²ç¨‹é‡è¤‡: {len(duplicate_stats['courses']['duplicates'])} çµ„")
            print(f"  ç« ç¯€é‡è¤‡: {len(duplicate_stats['chapters']['duplicates'])} çµ„") 
            print(f"  å–®å…ƒé‡è¤‡: {len(duplicate_stats['units']['duplicates'])} çµ„")
        
        print(f"\nğŸ“Š å„èª²ç¨‹çµ±è¨ˆè©³æƒ…:")
        print("="*60)
        
        for i, course_name in enumerate(course_order, 1):
            stats = course_stats[course_name]
            
            # ç²å–èª²ç¨‹çš„ä¾†æºSheetåç¨±
            source_sheet = course_to_sheet.get(course_name, '')
            display_name = f"{course_name}ï¼ˆ{source_sheet}ï¼‰" if source_sheet else course_name
            
            # è¨ˆç®—è©²èª²ç¨‹çš„è³‡æºé‡è¤‡å¼•ç”¨æƒ…æ³
            course_file_paths = stats['resources']
            course_multiple_reference = []
            course_saved_uploads = 0
            
            for file_path in course_file_paths:
                reference_count = file_path_references.get(file_path, 1)
                if reference_count > 1:
                    course_multiple_reference.append({
                        'æª”æ¡ˆè·¯å¾‘': file_path,
                        'æª”æ¡ˆåç¨±': os.path.basename(file_path),
                        'å¼•ç”¨æ¬¡æ•¸': reference_count
                    })
                    course_saved_uploads += reference_count - 1
            
            course_detail = {
                'order': i,
                'name': course_name,
                'chapters': len(stats['chapters']),
                'units': len(stats['units']),
                'activities': len(stats['activities']),
                'resources': len(stats['resources']),
                'html_files': stats['html_files'],
                'video_activities': stats['video_activities'],  # ä¿®æ­£
                'multiple_reference_resources': len(course_multiple_reference),
                'saved_uploads': course_saved_uploads
            }
            
            report_data['course_details'].append(course_detail)
            
            # å¯«å…¥æ—¥èªŒæª”æ¡ˆ
            log_file.write(f"{i}. {display_name}\n")
            log_file.write(f"   èª²ç¨‹åŒ¯ç¸½: ç« ç¯€æ•¸={len(stats['chapters'])}, å–®å…ƒæ•¸={len(stats['units'])}, å­¸ç¿’æ´»å‹•æ•¸={len(stats['activities'])}, è³‡æºæ•¸={len(stats['resources'])}\n")
            log_file.write(f"   ç´°ç¯€åŒ¯ç¸½: HTMLç·šä¸Šé€£çµæ•¸={stats['html_files']}, å½±éŸ³æ•™ææ´»å‹•æ•¸={stats['video_activities']}, è¢«å¤šå€‹å­¸ç¿’æ´»å‹•å¼•ç”¨çš„è³‡æºæ•¸={len(course_multiple_reference)}, ç¯€çœé‡è¤‡ä¸Šå‚³çš„è³‡æºæ•¸={course_saved_uploads}\n")  # ä¿®æ­£
            
            if course_multiple_reference:
                log_file.write(f"   é‡è¤‡å¼•ç”¨è³‡æºè©³æƒ…:\n")
                for res in course_multiple_reference:
                    log_file.write(f"     - {res['æª”æ¡ˆåç¨±']}: è¢«{res['å¼•ç”¨æ¬¡æ•¸']}å€‹å­¸ç¿’æ´»å‹•å¼•ç”¨\n")
            
            log_file.write("\n")
            
            # æ§åˆ¶å°è¼¸å‡º
            print(f"{i:2d}. {display_name}")
            print(f"    ğŸ“Š èª²ç¨‹åŒ¯ç¸½: ç« ç¯€={len(stats['chapters'])}, å–®å…ƒ={len(stats['units'])}, æ´»å‹•={len(stats['activities'])}, è³‡æº={len(stats['resources'])}")
            print(f"    ğŸ” ç´°ç¯€åŒ¯ç¸½: HTMLé€£çµ={stats['html_files']}, å½±éŸ³æ•™æ={stats['video_activities']}, é‡è¤‡å¼•ç”¨è³‡æº={len(course_multiple_reference)}, ç¯€çœä¸Šå‚³={course_saved_uploads}")  # ä¿®æ­£
            if course_multiple_reference:
                # é¡¯ç¤ºå‰3å€‹é‡è¤‡å¼•ç”¨çš„è³‡æº
                resource_names = []
                for r in course_multiple_reference[:3]:
                    resource_names.append(f"{r['æª”æ¡ˆåç¨±']}({r['å¼•ç”¨æ¬¡æ•¸']})")
                resources_text = ', '.join(resource_names)
                
                if len(course_multiple_reference) > 3:
                    resources_text += f"... ç­‰{len(course_multiple_reference)}å€‹"
                
                print(f"    ğŸ”„ é‡è¤‡å¼•ç”¨: {resources_text}")
            print()
        
        # ç”ŸæˆJSONæ ¼å¼çš„è©³ç´°å ±å‘Š
        json_filename = f"log/todolist_course_analysis_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as json_file:
            json.dump(report_data, json_file, ensure_ascii=False, indent=2)
        
        log_file.write("="*80 + "\n")
        log_file.write("å ±å‘Šç”Ÿæˆå®Œæˆ\n")
        log_file.write(f"JSONè©³ç´°å ±å‘Š: {json_filename}\n")
        
        print(f"ğŸ“ è©³ç´°çµ±è¨ˆå ±å‘Šå·²ç”Ÿæˆ:")
        print(f"   ğŸ“„ æ—¥èªŒæª”æ¡ˆ: {log_filename}")
        print(f"   ğŸ“‹ JSONå ±å‘Š: {json_filename}")
        
        return report_data

def find_header_positions(df):
    """å‹•æ…‹æŸ¥æ‰¾è¡¨é ­ä½ç½®"""
    header_info = {
        'header_row': -1,
        'course_col': -1,
        'chapter_col': -1,
        'unit_cols': [],
        'activity_col': -1,
        'type_col': -1,
        'path_col': -1,
        'system_path_col': -1  # æ–°å¢ç³»çµ±è·¯å¾‘æ¬„ä½
    }
    
    # æŸ¥æ‰¾è¡¨é ­è¡Œï¼ˆåŒ…å«"èª²ç¨‹åç¨±"çš„è¡Œï¼‰
    for row_idx in range(min(15, len(df))):
        row_values = df.iloc[row_idx].astype(str).str.strip()
        if 'èª²ç¨‹åç¨±' in row_values.values:
            header_info['header_row'] = row_idx
            break
    
    if header_info['header_row'] == -1:
        print("âŒ æ‰¾ä¸åˆ°åŒ…å«'èª²ç¨‹åç¨±'çš„è¡¨é ­è¡Œ")
        return header_info
    
    # æŸ¥æ‰¾å„æ¬„ä½ä½ç½®
    header_row = df.iloc[header_info['header_row']].astype(str).str.strip()
    
    for col_idx, col_value in enumerate(header_row):
        if col_value == 'èª²ç¨‹åç¨±':
            header_info['course_col'] = col_idx
        elif col_value == 'ç« ç¯€':
            header_info['chapter_col'] = col_idx
        elif col_value.startswith('å–®å…ƒ'):
            header_info['unit_cols'].append(col_idx)
        elif col_value == 'å­¸ç¿’æ´»å‹•':
            header_info['activity_col'] = col_idx
    
    # æŸ¥æ‰¾é¡å‹ã€è·¯å¾‘å’Œç³»çµ±è·¯å¾‘æ¬„ä½ï¼ˆå¯èƒ½åœ¨å…¶ä»–è¡Œï¼‰
    for row_idx in range(min(15, len(df))):
        row_values = df.iloc[row_idx].astype(str).str.strip()
        for col_idx, col_value in enumerate(row_values):
            if col_value == 'é¡å‹' and header_info['type_col'] == -1:
                header_info['type_col'] = col_idx
            elif col_value == 'è·¯å¾‘' and header_info['path_col'] == -1:
                header_info['path_col'] = col_idx
            elif col_value == 'ç³»çµ±è·¯å¾‘' and header_info['system_path_col'] == -1:
                header_info['system_path_col'] = col_idx
    
    print(f"  âœ… è¡¨é ­è³‡è¨Š: è¡¨é ­è¡Œ={header_info['header_row']+1}, èª²ç¨‹åˆ—={header_info['course_col']+1}, "
          f"ç« ç¯€åˆ—={header_info['chapter_col']+1}, å–®å…ƒåˆ—={[c+1 for c in header_info['unit_cols']]}, "
          f"æ´»å‹•åˆ—={header_info['activity_col']+1}, é¡å‹åˆ—={header_info['type_col']+1}, "
          f"è·¯å¾‘åˆ—={header_info['path_col']+1}, ç³»çµ±è·¯å¾‘åˆ—={header_info['system_path_col']+1}")
    
    return header_info
    """å‹•æ…‹æŸ¥æ‰¾è¡¨é ­ä½ç½®"""
    header_info = {
        'header_row': -1,
        'course_col': -1,
        'chapter_col': -1,
        'unit_cols': [],
        'activity_col': -1,
        'type_col': -1,
        'path_col': -1,
        'system_path_col': -1  # æ–°å¢ç³»çµ±è·¯å¾‘æ¬„ä½
    }
    
    # æŸ¥æ‰¾è¡¨é ­è¡Œï¼ˆåŒ…å«"èª²ç¨‹åç¨±"çš„è¡Œï¼‰
    for row_idx in range(min(15, len(df))):
        row_values = df.iloc[row_idx].astype(str).str.strip()
        if 'èª²ç¨‹åç¨±' in row_values.values:
            header_info['header_row'] = row_idx
            break
    
    if header_info['header_row'] == -1:
        print("âŒ æ‰¾ä¸åˆ°åŒ…å«'èª²ç¨‹åç¨±'çš„è¡¨é ­è¡Œ")
        return header_info
    
    # æŸ¥æ‰¾å„æ¬„ä½ä½ç½®
    header_row = df.iloc[header_info['header_row']].astype(str).str.strip()
    
    for col_idx, col_value in enumerate(header_row):
        if col_value == 'èª²ç¨‹åç¨±':
            header_info['course_col'] = col_idx
        elif col_value == 'ç« ç¯€':
            header_info['chapter_col'] = col_idx
        elif col_value.startswith('å–®å…ƒ'):
            header_info['unit_cols'].append(col_idx)
        elif col_value == 'å­¸ç¿’æ´»å‹•':
            header_info['activity_col'] = col_idx
    
    # æŸ¥æ‰¾é¡å‹ã€è·¯å¾‘å’Œç³»çµ±è·¯å¾‘æ¬„ä½ï¼ˆå¯èƒ½åœ¨å…¶ä»–è¡Œï¼‰
    for row_idx in range(min(15, len(df))):
        row_values = df.iloc[row_idx].astype(str).str.strip()
        for col_idx, col_value in enumerate(row_values):
            if col_value == 'é¡å‹' and header_info['type_col'] == -1:
                header_info['type_col'] = col_idx
            elif col_value == 'è·¯å¾‘' and header_info['path_col'] == -1:
                header_info['path_col'] = col_idx
            elif col_value == 'ç³»çµ±è·¯å¾‘' and header_info['system_path_col'] == -1:
                header_info['system_path_col'] = col_idx
    
    print(f"  âœ… è¡¨é ­è³‡è¨Š: è¡¨é ­è¡Œ={header_info['header_row']+1}, èª²ç¨‹åˆ—={header_info['course_col']+1}, "
          f"ç« ç¯€åˆ—={header_info['chapter_col']+1}, å–®å…ƒåˆ—={[c+1 for c in header_info['unit_cols']]}, "
          f"æ´»å‹•åˆ—={header_info['activity_col']+1}, é¡å‹åˆ—={header_info['type_col']+1}, "
          f"è·¯å¾‘åˆ—={header_info['path_col']+1}, ç³»çµ±è·¯å¾‘åˆ—={header_info['system_path_col']+1}")
    
    return header_info

def create_extracted_excel(source_file, selected_sheets, timestamp):
    """å‰µå»ºæå–å¾Œçš„ Excel æª”æ¡ˆ"""
    output_filename = os.path.join('to_be_executed', f"todolist_extracted_{timestamp}.xlsx")
    
    all_result_data = []
    
    # é‡è¦ï¼šåœ¨è™•ç†æ‰€æœ‰ sheets ä¹‹å‰ï¼Œé‡ç½®å…¨åŸŸé‡è¤‡åç¨±ç®¡ç†å™¨å’Œç« ç¯€è¨ˆæ•¸å™¨
    from sub_todolist_result import duplicate_manager, course_chapter_counter
    duplicate_manager.__init__()  # é‡ç½®ç®¡ç†å™¨
    course_chapter_counter.clear()  # é‡ç½®ç« ç¯€è¨ˆæ•¸å™¨
    
    # è™•ç†æ¯å€‹é¸ä¸­çš„ sheet
    for sheet_name in selected_sheets:
        print(f"\nğŸ“‹ æ­£åœ¨è™•ç† sheet: {sheet_name}")
        
        try:
            # è®€å– sheet è³‡æ–™
            df = pd.read_excel(source_file, sheet_name=sheet_name, header=None)
            
            # æŸ¥æ‰¾è¡¨é ­ä½ç½®
            header_info = find_header_positions(df)
            
            # ä½¿ç”¨ sub_todolist_result è™•ç†è³‡æ–™
            result_data = process_sheet_data(df, sheet_name, header_info)
            
            all_result_data.extend(result_data)
            
            print(f"  âœ… è™•ç†å®Œæˆ: {len(result_data)} æ¢è¨˜éŒ„")
            
        except Exception as e:
            print(f"  âŒ è™•ç† sheet {sheet_name} æ™‚å‡ºéŒ¯: {e}")
    
    # ä½¿ç”¨ sub_todolist_resource å¾ result_data ä¸­æå–è³‡æºï¼ˆå»é‡ç‰ˆæœ¬ï¼‰
    print(f"\nğŸ“¦ æ­£åœ¨æå–è³‡æºæª”æ¡ˆæ¸…å–®...")
    all_resource_data = extract_resources_from_result(all_result_data)
    
    # ç²å–è³‡æºçµ±è¨ˆè³‡è¨Š
    resource_stats = get_resource_statistics(all_result_data)
    
    # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    print(f"\nğŸ“Š è³‡æºè™•ç†çµ±è¨ˆ:")
    print(f"  - æœ‰æª”æ¡ˆè·¯å¾‘çš„å­¸ç¿’æ´»å‹•: {resource_stats['total_activities_with_files']} å€‹")
    print(f"  - å”¯ä¸€æª”æ¡ˆè·¯å¾‘: {resource_stats['unique_file_paths']} å€‹")
    print(f"  - ç”Ÿæˆè³‡æºè¨˜éŒ„: {len(all_resource_data)} ç­†")
    
    if resource_stats['multiple_reference_files'] > 0:
        print(f"  - é‡è¤‡å¼•ç”¨çš„æª”æ¡ˆ: {resource_stats['multiple_reference_files']} å€‹")
        print(f"    ğŸ’¡ é€™äº›æª”æ¡ˆåªæœƒä¸Šå‚³ä¸€æ¬¡ï¼Œä½†è¢«å¤šå€‹å­¸ç¿’æ´»å‹•å¼•ç”¨")
    
    # å‰µå»º Excel æª”æ¡ˆ
    with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:
        # 1. è¤‡è£½åŸå§‹è³‡æ–™åˆ° Ori_document sheet
        print(f"\nğŸ“„ æ­£åœ¨è¤‡è£½åŸå§‹è³‡æ–™...")
        all_ori_data = []
        
        for sheet_name in selected_sheets:
            try:
                # ä½¿ç”¨ header=None è®€å–ï¼Œä¿æŒåŸå§‹æ ¼å¼
                df = pd.read_excel(source_file, sheet_name=sheet_name, header=None)
                # æ·»åŠ ä¾†æºæ¨™è¨˜åˆ—
                df['åŸå§‹Sheet'] = sheet_name
                all_ori_data.append(df)
            except Exception as e:
                print(f"  âŒ è¤‡è£½ sheet {sheet_name} æ™‚å‡ºéŒ¯: {e}")
        
        if all_ori_data:
            # å‚ç›´åˆä½µï¼Œä¿æŒåŸå§‹çµæ§‹
            combined_df = pd.concat(all_ori_data, ignore_index=True, sort=False)
            combined_df.to_excel(writer, sheet_name='Ori_document', index=False, header=False)
            print(f"  âœ… å·²ä¿å­˜åŸå§‹è³‡æ–™ ({len(combined_df)} è¡Œ)")
        
        # 2. ä¿å­˜ Result sheet
        if all_result_data:
            result_df = pd.DataFrame(all_result_data)
            result_df.to_excel(writer, sheet_name='Result', index=False)
            print(f"  âœ… å·²ä¿å­˜ Result è³‡æ–™ ({len(all_result_data)} æ¢è¨˜éŒ„)")
        else:
            # å‰µå»ºç©ºçš„ Result sheet
            result_columns = ['é¡å‹', 'åç¨±', 'ID', 'æ‰€å±¬èª²ç¨‹', 'æ‰€å±¬èª²ç¨‹ID', 'æ‰€å±¬ç« ç¯€', 'æ‰€å±¬ç« ç¯€ID', 
                            'æ‰€å±¬å–®å…ƒ', 'æ‰€å±¬å–®å…ƒID', 'å­¸ç¿’æ´»å‹•é¡å‹', 'ç¶²å€è·¯å¾‘', 'æª”æ¡ˆè·¯å¾‘', 'è³‡æºID', 'æœ€å¾Œä¿®æ”¹æ™‚é–“', 'ä¾†æºSheet']
            result_df = pd.DataFrame(columns=pd.Index(result_columns))
            result_df.to_excel(writer, sheet_name='Result', index=False)
            print(f"  âœ… å·²å‰µå»ºç©ºçš„ Result sheet")
        
        # 3. ä¿å­˜ Resource sheet
        if all_resource_data:
            resource_df = pd.DataFrame(all_resource_data)
            resource_df.to_excel(writer, sheet_name='Resource', index=False)
            print(f"  âœ… å·²ä¿å­˜ Resource è³‡æ–™ ({len(all_resource_data)} å€‹å”¯ä¸€è³‡æº)")
        else:
            # å‰µå»ºç©ºçš„ Resource sheet ä½†åŒ…å«æ¨™é¡Œè¡Œ
            resource_columns = ['æª”æ¡ˆåç¨±', 'æª”æ¡ˆè·¯å¾‘', 'è³‡æºID', 'æœ€å¾Œä¿®æ”¹æ™‚é–“', 'ä¾†æºSheet', 'å¼•ç”¨å­¸ç¿’æ´»å‹•æ•¸', 'å¼•ç”¨æ´»å‹•åˆ—è¡¨']
            resource_df = pd.DataFrame(columns=pd.Index(resource_columns))
            resource_df.to_excel(writer, sheet_name='Resource', index=False)
            print(f"  âœ… å·²å‰µå»ºç©ºçš„ Resource sheet")
    
    print(f"\nğŸ‰ æå–å®Œæˆï¼æª”æ¡ˆå·²ç”Ÿæˆ: {output_filename}")
    
    # é¡¯ç¤ºé‡è¤‡è³‡æºç¯€çœçš„è³‡è¨Š
    if resource_stats['total_activities_with_files'] > resource_stats['unique_file_paths']:
        saved_count = resource_stats['total_activities_with_files'] - resource_stats['unique_file_paths']
        print(f"\nğŸ’¾ è³‡æºå»é‡æ•ˆæœ:")
        print(f"  - åŸæœ¬éœ€è¦: {resource_stats['total_activities_with_files']} å€‹è³‡æºä¸Šå‚³")
        print(f"  - å»é‡å¾Œéœ€è¦: {resource_stats['unique_file_paths']} å€‹è³‡æºä¸Šå‚³")
        print(f"  - ç¯€çœä¸Šå‚³: {saved_count} å€‹é‡è¤‡è³‡æº")
    
    # ç”Ÿæˆè©³ç´°çš„èª²ç¨‹çµ±è¨ˆå ±å‘Š
    print(f"\nğŸ“‹ æ­£åœ¨ç”Ÿæˆè©³ç´°çµ±è¨ˆå ±å‘Š...")
    analyze_course_statistics(all_result_data, all_resource_data, timestamp)
    
    return output_filename

def main():
    """ä¸»å‡½æ•¸ - GUIçµ‚ç«¯å‹å–„ç‰ˆæœ¬"""
    print("ğŸš€ TronClass èª²ç¨‹çµæ§‹è³‡æ–™æå–å™¨ - è³‡æºå»é‡ç‰ˆæœ¬")
    print("âœ¨ æ–°åŠŸèƒ½ï¼šç›¸åŒæª”æ¡ˆè·¯å¾‘çš„è³‡æºåªæœƒç”Ÿæˆä¸€ç­†è¨˜éŒ„")
    print("=" * 55)
    print("ğŸ” æ­£åœ¨æª¢æŸ¥ to_be_executed ç›®éŒ„...")
    
    # 1. ç²å– analyzed æª”æ¡ˆ
    files = get_analyzed_files()
    if not files:
        print("âŒ æœªæ‰¾åˆ° analyzed æª”æ¡ˆï¼Œè…³æœ¬çµæŸ")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(files)} å€‹ analyzed æª”æ¡ˆ")
    
    # 2. è®“ç”¨æˆ¶é¸æ“‡æª”æ¡ˆ
    selected_file = select_file(files)
    print(f"\nâœ… å·²é¸æ“‡æª”æ¡ˆ: {os.path.basename(selected_file)}")
    
    # 3. ç²å– sheet åç¨±
    print(f"\nğŸ” æ­£åœ¨è®€å– Excel æª”æ¡ˆçµæ§‹...")
    sheet_names = get_sheet_names(selected_file)
    if not sheet_names:
        print("âŒ ç„¡æ³•è®€å– Excel æª”æ¡ˆï¼Œè…³æœ¬çµæŸ")
        return
    
    print(f"âœ… æª”æ¡ˆåŒ…å« {len(sheet_names)} å€‹ sheet")
    
    # 4. è®“ç”¨æˆ¶é¸æ“‡ sheet
    selected_sheets = select_sheet(sheet_names)
    print(f"\nâœ… å·²é¸æ“‡ sheet: {', '.join(selected_sheets)}")
    
    # 5. æå–æ™‚é–“æˆ³
    timestamp = extract_timestamp(selected_file)
    
    # 6. å‰µå»ºæå–å¾Œçš„ Excel æª”æ¡ˆ
    print(f"\nğŸ”„ æ­£åœ¨è™•ç†æ•¸æ“šä¸¦ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆ...")
    print("ğŸ”„ é€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…...")
    output_file = create_extracted_excel(selected_file, selected_sheets, timestamp)
    print(f"\nâœ… è¼¸å‡ºæª”æ¡ˆç”Ÿæˆå®Œæˆ: {output_file}")
    
    print(f"\nğŸ“Š ç”Ÿæˆçš„æª”æ¡ˆåŒ…å«ä»¥ä¸‹ sheet:")
    print("  1. Ori_document - åŸå§‹è³‡æ–™")
    print("  2. Result - æå–çš„çµæ§‹åŒ–è³‡æ–™")
    print("  3. Resource - éœ€è¦ä¸Šå‚³çš„è³‡æºæª”æ¡ˆæ¸…å–®ï¼ˆå·²å»é‡ï¼‰")
    
    print(f"\nğŸ’¡ æ”¹é€²èªªæ˜:")
    print("  âœ… å·²è‡ªå‹•æå–èª²ç¨‹ã€ç« ç¯€ã€å–®å…ƒã€å­¸ç¿’æ´»å‹•çš„å±¤ç´šé—œä¿‚")
    print("  âœ… å·²è‡ªå‹•è­˜åˆ¥ç·šä¸Šé€£çµå’Œæª”æ¡ˆè·¯å¾‘")
    print("  âœ… å·²è‡ªå‹•ç”Ÿæˆè³‡æºä¸Šå‚³æ¸…å–®ï¼ˆç›¸åŒæª”æ¡ˆè·¯å¾‘å»é‡ï¼‰")
    print("  âœ… æ”¯æ´å‹•æ…‹æ¬„ä½ä½ç½®è­˜åˆ¥")
    print("  âœ… æ”¯æ´å¤šç¨®æª”æ¡ˆçµæ§‹")
    print("  ğŸ†• Resource è¡¨æ–°å¢æ¬„ä½ï¼šå¼•ç”¨å­¸ç¿’æ´»å‹•æ•¸ã€å¼•ç”¨æ´»å‹•åˆ—è¡¨")
    print("  ğŸ†• ç›¸åŒæª”æ¡ˆè·¯å¾‘çš„è³‡æºåªç”Ÿæˆä¸€ç­†è¨˜éŒ„ï¼Œé¿å…é‡è¤‡ä¸Šå‚³")

if __name__ == "__main__":
    main()